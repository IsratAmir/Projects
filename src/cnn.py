# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sjo-ksn59Hyya6VlPpDrTc7vWCXPNVJW
"""

import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt

# 1) Reproducibility
SEED = 42
tf.keras.utils.set_random_seed(SEED)

# 2) Load CIFAR-10 with clean splits (val from train, test untouched)
(ds_train, ds_val, ds_test), ds_info = tfds.load(
    "cifar10",
    split=["train[10%:]", "train[:10%]", "test"],
    as_supervised=True,
    with_info=True
)

NUM_CLASSES = ds_info.features["label"].num_classes
IMG_SIZE = (32, 32)
BATCH_SIZE = 128

# 3) Preprocessing + augmentation
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomTranslation(0.1, 0.1),
    tf.keras.layers.RandomZoom(0.1),
], name="augmentation")

def preprocess(image, label, training=False):
    image = tf.cast(image, tf.float32) / 255.0
    if training:
        image = data_augmentation(image)
    return image, label

AUTOTUNE = tf.data.AUTOTUNE

ds_train = (ds_train
            .shuffle(10_000, seed=SEED)
            .map(lambda x, y: preprocess(x, y, training=True), num_parallel_calls=AUTOTUNE)
            .batch(BATCH_SIZE)
            .prefetch(AUTOTUNE))

ds_val = (ds_val
          .map(lambda x, y: preprocess(x, y, training=False), num_parallel_calls=AUTOTUNE)
          .batch(BATCH_SIZE)
          .prefetch(AUTOTUNE))

ds_test = (ds_test
           .map(lambda x, y: preprocess(x, y, training=False), num_parallel_calls=AUTOTUNE)
           .batch(BATCH_SIZE)
           .prefetch(AUTOTUNE))

# 4) A more modern CNN: Conv-BN-ReLU blocks + dropout
def conv_block(x, filters, drop=0.0):
    x = tf.keras.layers.Conv2D(filters, 3, padding="same", use_bias=False)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    x = tf.keras.layers.Conv2D(filters, 3, padding="same", use_bias=False)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    if drop > 0:
        x = tf.keras.layers.Dropout(drop)(x)
    return x

inputs = tf.keras.Input(shape=(32, 32, 3))
x = conv_block(inputs, 32, drop=0.1)
x = conv_block(x, 64, drop=0.2)
x = conv_block(x, 128, drop=0.3)

x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dense(128, activation="relu")(x)
x = tf.keras.layers.Dropout(0.4)(x)
outputs = tf.keras.layers.Dense(NUM_CLASSES, activation="softmax")(x)

model = tf.keras.Model(inputs, outputs)
model.summary()

# 5) Compile (label is integer => SparseCategoricalCrossentropy is correct)
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=["accuracy"]
)

# 6) Callbacks
callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor="val_accuracy", patience=5, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=2),
    tf.keras.callbacks.ModelCheckpoint("best_cifar10_model.keras", monitor="val_accuracy", save_best_only=True),
]

history = model.fit(ds_train, validation_data=ds_val, epochs=30, callbacks=callbacks)

# 7) Evaluate
test_loss, test_acc = model.evaluate(ds_test)
print(f"Test accuracy: {test_acc:.4f}")

# 8) Plot training curves
plt.plot(history.history["loss"], label="train_loss")
plt.plot(history.history["val_loss"], label="val_loss")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.legend()
plt.show()